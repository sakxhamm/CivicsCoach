// backend/src/controllers/debateController.js
const { buildChainMessages } = require('../prompts/chainOfThoughtPrompt');
const { callGemini } = require('../services/geminiService');
const { safeParseJSONMaybe, validateDebateSchema } = require('../utils/jsonValidator');

// Load corpus data for retrieval
const corpus = require('../../data/corpus_chunks.json');

// Simple similarity-based retrieval (for demo purposes)
function retrieveChunks(query, topK = 4) {
  // Simple keyword matching for demo
  const queryLower = query.toLowerCase();
  const scoredChunks = corpus.map(chunk => {
    const textLower = chunk.text.toLowerCase();
    let score = 0;
    
    // Simple keyword scoring
    const keywords = queryLower.split(' ');
    keywords.forEach(keyword => {
      if (textLower.includes(keyword)) score += 1;
    });
    
    return { ...chunk, score };
  });
  
  // Sort by score and return top K
  return scoredChunks
    .sort((a, b) => b.score - a.score)
    .slice(0, topK)
    .map(chunk => ({ 
      id: chunk.id, 
      text: chunk.text, 
      metadata: chunk.metadata 
    }));
}

async function generate(req, res) {
  try {
    const { 
      query, 
      topK = 4, 
      metric = 'cosine', 
      proficiency = 'intermediate', 
      temperature = 0.2, 
      top_p = 1.0, 
      useCoT = true 
    } = req.body;

    if (!query) {
      return res.status(400).json({ 
        ok: false, 
        error: 'Query is required' 
      });
    }

    // 1) RETRIEVE relevant chunks
    const retrievedChunks = retrieveChunks(query, topK);

    // 2) Build messages with chain-of-thought prompt
    const messages = buildChainMessages({ 
      audience: proficiency, 
      topic: query, 
      retrievedChunks, 
      minCitations: 2, 
      proficiency, 
      examples: true 
    });

    if (!useCoT) {
      messages[0].content = messages[0].content.replace(
        'You MAY use internal step-by-step reasoning to improve accuracy, BUT DO NOT reveal the chain-of-thought.',
        'Do NOT use internal step-by-step reasoning. Answer directly.'
      );
    }

    // 3) Call Gemini API
    const llmResp = await callGemini({ 
      messages, 
      temperature, 
      top_p 
    });

    // 4) Parse & validate JSON
    const parsed = safeParseJSONMaybe(llmResp.text);
    if (!parsed.ok) {
      return res.status(500).json({ 
        ok: false, 
        error: 'Could not parse model output', 
        details: parsed.error, 
        raw: llmResp.text 
      });
    }

    const valid = validateDebateSchema(parsed.data);
    if (!valid.ok) {
      return res.status(500).json({ 
        ok: false, 
        error: 'Schema validation failed', 
        details: valid.error, 
        parsed: parsed.data 
      });
    }

    // 5) Return structured response with metadata
    res.json({ 
      ok: true, 
      data: parsed.data, 
      metadata: {
        retrievedChunks: retrievedChunks.length,
        useCoT,
        temperature,
        top_p,
        tokens: llmResp.usage || { input: 0, output: 0 }
      },
      raw: llmResp.raw 
    });
  } catch (err) {
    console.error('Debate generation error:', err);
    
    // Handle rate limit specifically
    if (err.message.includes('Rate limit exceeded')) {
      // Return a demo response for rate limited cases
      const mockData = {
        stance: "This is a demo response. The actual AI service is currently rate limited. Please try again later or contact support for API access.",
        counterStance: "In a real implementation, this would contain the opposing viewpoint generated by the AI model using Chain of Thought reasoning.",
        citations: [
          {
            id: "demo",
            source: "Demo Mode",
            snippet: "This is a placeholder citation. Real citations would be retrieved from the constitutional corpus."
          }
        ],
        quiz: [
          {
            q: "This is a demo quiz question. Real questions would be generated by the AI model.",
            options: ["Option A", "Option B", "Option C", "Option D"],
            answerIndex: 0
          }
        ]
      };
      
      return res.json({ 
        ok: true, 
        data: mockData, 
        metadata: {
          retrievedChunks: 0,
          useCoT: req.body.useCoT || true,
          temperature: req.body.temperature || 0.2,
          top_p: req.body.top_p || 1.0,
          tokens: { input: 100, output: 200, total: 300 },
          demo: true
        },
        raw: { demo: true, message: "Rate limited - using demo response" }
      });
    }
    
    res.status(500).json({ 
      ok: false, 
      error: err.message 
    });
  }
}

module.exports = { generate };
